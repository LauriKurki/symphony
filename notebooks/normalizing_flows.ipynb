{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import distrax\n",
    "import haiku as hk\n",
    "import optax\n",
    "import seaborn as sns\n",
    "import jax.config\n",
    "jax.config.update(\"jax_enable_x64\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# true_dist = distrax.MixtureOfTwo(0.5,\n",
    "#                                  distrax.Normal(loc=1., scale=0.1),\n",
    "#                                  distrax.Normal(loc=3., scale=0.1)) \n",
    "true_dist = distrax.Normal(loc=2., scale=1e-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = jax.random.PRNGKey(1234)\n",
    "num_bins = 10\n",
    "num_layers = 5\n",
    "num_param_mlp_layers = 2\n",
    "range_min = -1\n",
    "range_max = 10\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RQSpline(hk.Module):\n",
    "\n",
    "    def __init__(self, num_bins, range_min, range_max, num_layers: int):\n",
    "        super().__init__()\n",
    "        self.num_bins = num_bins\n",
    "        self.range_min = range_min\n",
    "        self.range_max = range_max\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "    def create_flow(self, conditioning):\n",
    "        layers = []\n",
    "        for _ in range(self.num_layers):\n",
    "            param_dims = self.num_bins * 3 + 1\n",
    "            params = hk.nets.MLP(\n",
    "                [param_dims] * num_param_mlp_layers,\n",
    "                activate_final=False,\n",
    "                w_init=hk.initializers.RandomNormal(1e-4),\n",
    "                b_init=hk.initializers.RandomNormal(1e-4),\n",
    "            )(conditioning)\n",
    "            layer = distrax.RationalQuadraticSpline(params, self.range_min, self.range_max, boundary_slopes='unconstrained', min_bin_size=1e-2)\n",
    "            layers.append(layer)\n",
    "\n",
    "        flow = distrax.Inverse(distrax.Chain(layers))\n",
    "        return flow\n",
    "\n",
    "    def create_distribution(self, conditioning, inverse_temperature: float = 1.):\n",
    "        flow = self.create_flow(conditioning)\n",
    "        \n",
    "        mean = (self.range_max + self.range_min) / 2\n",
    "        std = (self.range_max - self.range_min) / (20 * inverse_temperature)\n",
    "        base_distribution = distrax.Independent(\n",
    "            distrax.ClippedNormal(mean, std, minimum=self.range_min, maximum=self.range_max),\n",
    "            reinterpreted_batch_ndims=0)\n",
    "\n",
    "        dist = distrax.Transformed(base_distribution, flow)\n",
    "        return dist\n",
    "\n",
    "    def forward(self, samples, conditioning):\n",
    "        flow = self.create_flow(conditioning)\n",
    "        return flow.forward(samples)\n",
    "    \n",
    "    def log_prob(self, samples, conditioning):\n",
    "        assert conditioning.shape[:-1] == samples.shape[:-1]\n",
    "        dist = self.create_distribution(conditioning)\n",
    "        return dist.log_prob(samples)\n",
    "\n",
    "    def sample(self, conditioning, inverse_temperature):\n",
    "        dist = self.create_distribution(conditioning, inverse_temperature)\n",
    "        rng = hk.next_rng_key()\n",
    "        return dist.sample(seed=rng, sample_shape=conditioning.shape[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@hk.without_apply_rng\n",
    "@hk.transform\n",
    "def log_prob_fn(samples, conditioning):\n",
    "    return hk.vmap(RQSpline(num_bins, range_min, range_max, num_layers).log_prob, split_rng=False)(samples, conditioning)\n",
    "\n",
    "\n",
    "@hk.transform\n",
    "def sample_fn(conditioning, inverse_temperature: float):\n",
    "    return hk.vmap(lambda condition: RQSpline(num_bins, range_min, range_max, num_layers).sample(condition, inverse_temperature), split_rng=True)(conditioning)\n",
    "\n",
    "\n",
    "@hk.without_apply_rng\n",
    "@hk.transform\n",
    "def forward_fn(base_samples, conditioning):\n",
    "    return hk.vmap(RQSpline(num_bins, range_min, range_max, num_layers).forward, split_rng=False)(base_samples, conditioning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def loss_fn(params, true_samples, true_conditioning):\n",
    "    loss = -jnp.mean(log_prob_fn.apply(params, true_samples, true_conditioning))\n",
    "    return loss\n",
    "\n",
    "dummy_samples = jnp.zeros((batch_size, 1))\n",
    "dummy_conditioning = jnp.ones_like(dummy_samples)\n",
    "params = log_prob_fn.init(rng, dummy_samples, dummy_conditioning)\n",
    "tx = optax.chain(optax.adam(1e-4))\n",
    "# tx = optax.sgd(1e-2)\n",
    "opt_state = tx.init(params)\n",
    "\n",
    "@jax.jit\n",
    "def train_step(params, opt_state, true_samples, true_conditioning):\n",
    "    grads = jax.grad(loss_fn)(params, true_samples, true_conditioning)\n",
    "    updates, opt_state = tx.update(grads, opt_state)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return params, opt_state\n",
    "\n",
    "\n",
    "losses = []\n",
    "for step in range(10000):\n",
    "    step_rng, rng = jax.random.split(rng)\n",
    "    true_samples = true_dist.sample(seed=rng, sample_shape=(batch_size, 1))\n",
    "    true_conditioning = jnp.ones_like(true_samples)\n",
    "    params, opt_state = train_step(params, opt_state, true_samples, true_conditioning)\n",
    "    if step % 100 == 0:\n",
    "        loss = loss_fn(params, true_samples, true_conditioning)\n",
    "        losses.append(loss)\n",
    "        print(\"step:\", step, \"loss:\", loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"darkgrid\")\n",
    "plt.plot([x for x in range(10000) if x % 100 == 0], losses)\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_samples = true_dist.sample(seed=rng, sample_shape=(100, 1))\n",
    "true_conditioning = jnp.ones_like(true_samples)\n",
    "for inverse_temperature in [1.]:\n",
    "    samples = sample_fn.apply(params, rng, true_conditioning, inverse_temperature=inverse_temperature)\n",
    "    print(inverse_temperature, ':', samples.mean(), '+/-', samples.std())\n",
    "    # sns.set(style=\"darkgrid\")\n",
    "    # plt.hist(samples.flatten(), bins=100, color='C0', density=True)\n",
    "    # plt.hist(true_samples.flatten(), bins=100, color='C1', density=True)\n",
    "    # plt.show()\n",
    "    \n",
    "\n",
    "sns.set(style=\"darkgrid\")\n",
    "plt.hist(samples.flatten(), bins=100, color='C0', density=True)\n",
    "# plt.hist(true_samples.flatten(), bins=100, color='C1', density=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize log prob\n",
    "x = jnp.linspace(1.5, 2.5, 1000).reshape(-1, 1)\n",
    "y = jnp.ones_like(x)\n",
    "log_prob = log_prob_fn.apply(params, x, y)\n",
    "plt.plot(x, true_dist.log_prob(x), label=\"True\")\n",
    "plt.plot(x, log_prob, label=\"Estimated\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"log p(x)\")\n",
    "plt.title(\"Log Probability\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize flow\n",
    "sns.set(style=\"darkgrid\")\n",
    "x = jnp.linspace(range_min, range_max, 1000).reshape(-1, 1)\n",
    "y = forward_fn.apply(params, x, jnp.ones_like(x))\n",
    "plt.title(\"Flow\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f(x)\")\n",
    "plt.plot(x.flatten(), y.flatten(), label='true')\n",
    "plt.show();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
