{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "import optax\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "import e3nn_jax as e3nn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.colors\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from symphony import loss\n",
    "from symphony import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_positions = jnp.asarray([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]])\n",
    "# target_positions = jnp.asarray([[1., 0., 0.]])\n",
    "target_position_inverse_temperature = 10000.\n",
    "lmax = 3\n",
    "res_beta = 40\n",
    "res_alpha = 39"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_true_angular_coeffs = jax.vmap(\n",
    "    lambda pos: loss.target_position_to_log_angular_coeffs(\n",
    "        pos, target_position_inverse_temperature=target_position_inverse_temperature, lmax=lmax,\n",
    "    )\n",
    ")(target_positions)\n",
    "\n",
    "compute_grid_of_joint_distribution_fn = functools.partial(\n",
    "        models.compute_grid_of_joint_distribution,\n",
    "        res_beta=res_beta,\n",
    "        res_alpha=res_alpha,\n",
    "        quadrature=\"soft\",\n",
    "    )\n",
    "true_angular_dist = jax.vmap(\n",
    "    compute_grid_of_joint_distribution_fn,\n",
    ")(jnp.ones((target_positions.shape[0], 1)), log_true_angular_coeffs)\n",
    "true_angular_dist.grid_values = true_angular_dist.grid_values[:, 0, :, :]\n",
    "mean_true_angular_dist = e3nn.SphericalSignal(\n",
    "    grid_values=true_angular_dist.grid_values.mean(axis=0),\n",
    "    quadrature=true_angular_dist.quadrature\n",
    ")\n",
    "mean_true_angular_dist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coeffs_to_dist(log_predicted_angular_coeffs):\n",
    "    predicted_angular_dist = compute_grid_of_joint_distribution_fn(jnp.ones((1,)), log_predicted_angular_coeffs)\n",
    "    predicted_angular_dist = predicted_angular_dist[0, :, :]\n",
    "    return predicted_angular_dist\n",
    "\n",
    "\n",
    "def coeffs_to_logits(log_predicted_angular_coeffs):\n",
    "    predicted_angular_dist = coeffs_to_dist(log_predicted_angular_coeffs)\n",
    "    predicted_angular_logits = predicted_angular_dist.apply(models.safe_log)\n",
    "    return predicted_angular_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def factorized_position_loss(coeffs, dist) -> jnp.ndarray:\n",
    "    \"\"\"Computes the loss over position probabilities using separate losses for the radial and the angular components.\"\"\"\n",
    "    # Radial loss is simply the negative log-likelihood loss.\n",
    "    # loss_radial = -preds.globals.radial_logits.sum(axis=-1)\n",
    "    loss_radial = 0.\n",
    "\n",
    "    predicted_angular_logits = coeffs_to_logits(coeffs)\n",
    "    # The angular loss is the KL divergence between the predicted and the true angular distributions.\n",
    "    res_beta, res_alpha = (\n",
    "        predicted_angular_logits.res_beta,\n",
    "        predicted_angular_logits.res_alpha,\n",
    "    )\n",
    "\n",
    "    # jax.debug.print(\"max={x}\", x=true_angular_dist.grid_values.max())\n",
    "    # jax.debug.print(\"min={x}\", x=true_angular_dist.grid_values.min())\n",
    "    assert predicted_angular_logits.shape == (\n",
    "        res_beta,\n",
    "        res_alpha,\n",
    "    ), (predicted_angular_logits.shape, dist.shape)\n",
    "\n",
    "    loss_angular = loss.kl_divergence_on_spheres(\n",
    "        dist, predicted_angular_logits\n",
    "    )\n",
    "\n",
    "    loss_position = loss_radial + loss_angular\n",
    "    return loss_position\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def loss_fn(coeffs, target_dist):\n",
    "    return factorized_position_loss(coeffs, target_dist).mean()\n",
    "\n",
    "@functools.partial(jax.jit, static_argnames=(\"tx\", \"use_mean_dist\",))\n",
    "def step_fn(rng, coeffs, opt_state, tx, use_mean_dist):\n",
    "    if use_mean_dist:\n",
    "        target_dist = mean_true_angular_dist\n",
    "    else:\n",
    "        step_rng, rng = jax.random.split(rng)\n",
    "        target_index = jax.random.choice(step_rng, a=target_positions.shape[0])\n",
    "        target_dist = true_angular_dist[target_index]\n",
    "\n",
    "    grads = jax.grad(loss_fn)(coeffs, target_dist)\n",
    "    loss_val = loss_fn(coeffs, mean_true_angular_dist)\n",
    "    updates, opt_state = tx.update(grads, opt_state)\n",
    "    coeffs = optax.apply_updates(coeffs, updates)\n",
    "    return rng, coeffs, opt_state, loss_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_losses_by_hparams = {}\n",
    "all_steps_by_hparams = {}\n",
    "all_coeffs_by_hparams = {}\n",
    "\n",
    "for use_mean_dist in [True, False]:\n",
    "    for learning_rate in [1e1, 1e0, 1e-1, 1e-2, 1e-3, 1e-4]:\n",
    "        \n",
    "        rng = jax.random.PRNGKey(0)\n",
    "        init_coeffs = e3nn.normal(e3nn.s2_irreps(lmax=lmax), rng)\n",
    "        tx = optax.adam(learning_rate)\n",
    "        opt_state = tx.init(init_coeffs)\n",
    "\n",
    "        coeffs = init_coeffs\n",
    "        loss_val = float(loss_fn(coeffs, mean_true_angular_dist))\n",
    "\n",
    "        all_coeffs = []\n",
    "        all_steps = []\n",
    "        all_losses = []\n",
    "        for step in range(10000):\n",
    "            if step % 1000 == 0:\n",
    "                all_coeffs.append(coeffs)\n",
    "                all_steps.append(step)\n",
    "                all_losses.append(float(loss_val))\n",
    "\n",
    "            rng, coeffs, opt_state, loss_val = step_fn(rng, coeffs, opt_state, tx, use_mean_dist)\n",
    "            if step == 10000 - 1:\n",
    "                print(f\"step={step}: loss={loss_val}\")\n",
    "\n",
    "        all_losses_by_hparams[(use_mean_dist, learning_rate)] = all_losses\n",
    "        all_steps_by_hparams[(use_mean_dist, learning_rate)] = all_steps\n",
    "        all_coeffs_by_hparams[(use_mean_dist, learning_rate)] = all_coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"darkgrid\")\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(10, 6), sharey=True)\n",
    "for learning_rate in [1e0, 1e-1, 1e-2, 1e-3, 1e-4]:\n",
    "    sns.lineplot(x=all_steps_by_hparams[(False, learning_rate)], y=all_losses_by_hparams[(False, learning_rate)],\n",
    "                 ax=ax[0], hue=learning_rate, hue_norm=matplotlib.colors.LogNorm(vmin=1e-5, vmax=1e1), palette=\"viridis\")\n",
    "    sns.lineplot(x=all_steps_by_hparams[(True, learning_rate)], y=all_losses_by_hparams[(True, learning_rate)],\n",
    "                 ax=ax[1], hue=learning_rate, hue_norm=matplotlib.colors.LogNorm(vmin=1e-5, vmax=1e1), palette=\"viridis\")\n",
    "\n",
    "fig.suptitle(f\"Training Curves for Different Learning Rates\")\n",
    "ax[0].set_title(\"Training on Randomly Sampled Angular Distributions\")\n",
    "ax[1].set_title(\"Training on Mean Angular Distribution\")\n",
    "ax[0].set_xlabel(\"Steps\")\n",
    "ax[0].set_ylabel(\"Loss\")\n",
    "ax[1].set_xlabel(\"Steps\")\n",
    "ax[1].set_ylabel(\"Loss\")\n",
    "ax[0].set_yscale(\"log\")\n",
    "plt.legend(title=\"Learning Rate\")\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_mean_dist = True\n",
    "learning_rate = 1e-2\n",
    "\n",
    "all_steps = all_steps_by_hparams[(use_mean_dist, learning_rate)]\n",
    "all_coeffs = all_coeffs_by_hparams[(use_mean_dist, learning_rate)]\n",
    "# Make a slider plot of the angular distributions.\n",
    "fig = go.Figure()\n",
    "for step, coeffs in zip(all_steps, all_coeffs):\n",
    "    step_dist = coeffs_to_dist(coeffs)\n",
    "    step_trace = go.Surface(\n",
    "        step_dist.plotly_surface(),\n",
    "        colorscale=[[0, \"rgba(4, 59, 192, 0.)\"], [1, \"rgba(4, 59, 192, 1.)\"]],\n",
    "        showscale=True,\n",
    "        cmin=step_dist.grid_values.min().item(),\n",
    "        cmax=step_dist.grid_values.max().item(),\n",
    "        visible=False,\n",
    "    )\n",
    "    fig.add_trace(step_trace)\n",
    "\n",
    "# Make 0th trace visible\n",
    "fig.data[0].visible = True\n",
    "\n",
    "# Create and add slider\n",
    "steps = []\n",
    "for i in range(len(fig.data)):\n",
    "    step = dict(\n",
    "        method=\"update\",\n",
    "        args=[{\"visible\": [False] * len(fig.data)},\n",
    "              {\"title\": f\"Step: {all_steps[i]}\"}],  # layout attribute\n",
    "        label=f\"{all_steps[i]}\"\n",
    "    )\n",
    "    step[\"args\"][0][\"visible\"][i] = True  # Toggle i'th trace to \"visible\"\n",
    "    steps.append(step)\n",
    "\n",
    "sliders = [dict(\n",
    "    active=10,\n",
    "    currentvalue={\"prefix\": \"Step: \"},\n",
    "    pad={\"t\": 50},\n",
    "    steps=steps\n",
    ")]\n",
    "\n",
    "fig.update_layout(\n",
    "    sliders=sliders\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
