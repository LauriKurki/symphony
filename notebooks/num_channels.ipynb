{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import e3nn_jax as e3nn\n",
    "import haiku as hk\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "import plotly.graph_objects as go\n",
    "import plotly.subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import loss as loss_py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_radii = 1\n",
    "res_beta = 180\n",
    "res_alpha = 359"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_coeffs_to_probability_distribution(log_coeffs: e3nn.IrrepsArray) -> e3nn.SphericalSignal:\n",
    "    \"\"\"Converts irreps defining the logits to a probability distribution.\"\"\" \n",
    "    num_channels = log_coeffs.shape[0]\n",
    "    assert log_coeffs.shape == (num_channels, num_radii, log_coeffs.irreps.dim), log_coeffs.shape\n",
    "    \n",
    "    log_dist = e3nn.to_s2grid(log_coeffs, res_beta, res_alpha, quadrature=\"soft\", p_val=1, p_arg=-1)\n",
    "    assert log_dist.shape == (num_channels, num_radii, res_beta, res_alpha)\n",
    "\n",
    "    log_dist_max = jnp.max(log_dist.grid_values, axis=(-4, -3, -2, -1), keepdims=True)\n",
    "    log_dist_max = jax.lax.stop_gradient(log_dist_max)\n",
    "    log_dist = log_dist.apply(\n",
    "        lambda x: x - log_dist_max\n",
    "    )\n",
    "\n",
    "    dist = log_dist.apply(jnp.exp)\n",
    "    dist = dist / dist.integrate().array.sum()\n",
    "    dist.grid_values = dist.grid_values.sum(axis=-4)\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_coeffs(coeffs: e3nn.IrrepsArray) -> go.Figure:\n",
    "    sig = log_coeffs_to_probability_distribution(coeffs)\n",
    "    # num_radii is 1.\n",
    "    assert sig.grid_values.shape[0] == 1\n",
    "    sig.grid_values = sig.grid_values[0]\n",
    "    return plot_signal(sig)\n",
    " \n",
    "\n",
    "def plot_signal(sig: e3nn.SphericalSignal) -> go.Figure:\n",
    "    fig = plotly.subplots.make_subplots(rows=1, cols=2, specs=[[{'type': 'surface'}, {'type': 'surface'}]])\n",
    "    trace_1 = go.Surface(sig.plotly_surface(scale_radius_by_amplitude=False))\n",
    "    fig.add_trace(trace_1, row=1, col=1)\n",
    "\n",
    "    trace_2 = go.Surface(sig.plotly_surface(scale_radius_by_amplitude=True))\n",
    "    fig.add_trace(trace_2, row=1, col=2)\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_radius_weights = jnp.asarray([1.])\n",
    "log_true_angular_coeffs =  e3nn.IrrepsArray(\"4e\", jnp.array([[[1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0]]]))\n",
    "\n",
    "log_true_coeffs = true_radius_weights[None, :, None] * log_true_angular_coeffs\n",
    "true_dist = log_coeffs_to_probability_distribution(log_true_coeffs)\n",
    "\n",
    "plot_coeffs(log_true_coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence_on_spheres(\n",
    "    true_dist: e3nn.SphericalSignal,\n",
    "    log_predicted_dist: e3nn.SphericalSignal,\n",
    ") -> jnp.ndarray:\n",
    "    \"\"\"Compute the KL divergence between two distributions on the spheres.\"\"\"\n",
    "    assert true_dist.grid_values.shape == (num_radii, res_beta, res_alpha)\n",
    "    assert log_predicted_dist.grid_values.shape == (num_radii, res_beta, res_alpha)\n",
    "\n",
    "    # Now, compute the unnormalized predicted distribution over all spheres.\n",
    "    # Subtract the maximum value for numerical stability.\n",
    "    log_predicted_dist_max = jnp.max(log_predicted_dist.grid_values)\n",
    "    log_predicted_dist_max = jax.lax.stop_gradient(log_predicted_dist_max)\n",
    "    log_predicted_dist = log_predicted_dist.apply(\n",
    "        lambda x: x - log_predicted_dist_max\n",
    "    )\n",
    "\n",
    "    # Compute the cross-entropy including a normalizing factor to account for the fact that the predicted distribution is not normalized.\n",
    "    cross_entropy = -(true_dist * log_predicted_dist).integrate().array.sum()\n",
    "    normalizing_factor = jnp.log(\n",
    "        log_predicted_dist.apply(jnp.exp).integrate().array.sum()\n",
    "    )\n",
    "\n",
    "    # Compute the self-entropy of the true distribution.\n",
    "    self_entropy = (\n",
    "        -(true_dist * true_dist.apply(loss_py.safe_log)).integrate().array.sum()\n",
    "    )\n",
    "\n",
    "    # This should be non-negative, upto numerical precision.\n",
    "    return cross_entropy + normalizing_factor - self_entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alignment_loss(predicted_coeffs: e3nn.IrrepsArray) -> e3nn.IrrepsArray:\n",
    "    predicted_dist = log_coeffs_to_probability_distribution(predicted_coeffs)\n",
    "    log_predicted_dist = predicted_dist.apply(jnp.log)\n",
    "    loss = kl_divergence_on_spheres(true_dist, log_predicted_dist)\n",
    "    return e3nn.IrrepsArray(\"0e\", jnp.asarray([loss]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize predicted coefficients.\n",
    "rng = jax.random.PRNGKey(0)\n",
    "lmax = 1\n",
    "num_channels = 10\n",
    "irreps = e3nn.s2_irreps(lmax)\n",
    "init_coeffs = e3nn.normal(irreps, rng, leading_shape=(num_channels, num_radii))\n",
    "# init_coeffs = init_coeffs.mul_to_axis(num_radii)\n",
    "# init_coeffs = init_coeffs.mul_to_axis(num_channels)\n",
    "plot_coeffs(init_coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize coefficients to minimize the KL divergence.\n",
    "coeffs = init_coeffs\n",
    "tx = optax.adam(1e-2)\n",
    "opt_state = tx.init(coeffs)\n",
    "\n",
    "@jax.jit\n",
    "def train_step(coeffs, opt_state):\n",
    "    grad = e3nn.grad(alignment_loss)(\n",
    "        coeffs\n",
    "    )\n",
    "    loss = alignment_loss(coeffs)\n",
    "    updates, opt_state = tx.update(grad, opt_state)\n",
    "    coeffs = optax.apply_updates(coeffs, updates)\n",
    "    return coeffs, opt_state, loss\n",
    "\n",
    "for step in range(5000):\n",
    "    coeffs, opt_state, loss = train_step(coeffs, opt_state)\n",
    "    loss = loss.array.item()\n",
    "\n",
    "    if step % 100 == 0:\n",
    "        print(\"Loss at step {step} is {loss}\".format(step=step, loss=loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(coeffs, \"vs\", log_true_angular_coeffs)\n",
    "plot_coeffs(coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_coeffs(coeffs[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_coeffs(coeffs[1:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_coeffs(coeffs[2:3])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linearity of Projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 100\n",
    "sig1_irreps = e3nn.IrrepsArray(\"1o\", T * jnp.array([1.0, 0.0, 0.0]))\n",
    "sig1 = log_coeffs_to_probability_distribution(sig1_irreps)\n",
    "plot_signal(sig1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig2_irreps = e3nn.IrrepsArray(\"1o\", T * jnp.array([0.0, 0.0, 1.0]))\n",
    "sig2 = log_coeffs_to_probability_distribution(sig2_irreps)\n",
    "plot_signal(sig2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_signal((sig1 + sig2) / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig12_combined = log_coeffs_to_probability_distribution((sig1_irreps + sig2_irreps) / 2)\n",
    "plot_signal(sig12_combined)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
